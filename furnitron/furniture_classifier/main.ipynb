{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Furniture Classification Model\n",
    "\n",
    "This notebook demonstrates the process of loading, preprocessing, and training a machine learning model for classifying furniture items. The steps include data loading, tokenization, model training, and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using torch backend\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from datetime import datetime\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "\n",
    "print(\"Using torch backend\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seed RNGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 450\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "furniture_dataset = pd.read_csv(\"../data/model_input/dataset.csv\")\n",
    "flipkart_dataset = pd.read_csv(\"../data/model_input/flipkart.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of furniture names: 5605\n",
      "# of non-furniture names: 5605\n"
     ]
    }
   ],
   "source": [
    "# Extract unique furniture names from the furniture dataset.\n",
    "furniture_names = list(set(furniture_dataset[\"name\"]))\n",
    "\n",
    "# Extract unique product names from the flipkart dataset that are not categorized as furniture.\n",
    "other_product_names = list(flipkart_dataset[~flipkart_dataset[\"product_category_tree\"].str.startswith(\"[\\\"Furniture\")][\"product_name\"].unique())\n",
    "\n",
    "# Shuffle the list of non-furniture product names to randomize their order\n",
    "np.random.shuffle(other_product_names)\n",
    "\n",
    "# Truncate the list of non-furniture product names to the same length as the list of furniture names\n",
    "other_product_names = other_product_names[:len(furniture_names)]\n",
    "\n",
    "# Print the number of furniture and non-furniture names for comparison.\n",
    "print(f\"# of furniture names: {len(furniture_names)}\")\n",
    "print(f\"# of non-furniture names: {len(other_product_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FurnitureDataset(torch.utils.data.Dataset):\n",
    "  \"\"\"\n",
    "  A PyTorch Dataset class for furniture and non-furniture items.\n",
    "\n",
    "  This class is designed to handle datasets containing tokenized names of furniture and non-furniture products,\n",
    "  allowing it to be used with PyTorch's DataLoader for efficient batching during training or evaluation.\n",
    "\n",
    "  Attributes:\n",
    "  tokenized_furniture_names (dict): Tokenized names of furniture products.\n",
    "  tokenized_non_furniture_names (dict): Tokenized names of non-furniture products.\n",
    "  furniture_count (int): Number of furniture items in the dataset.\n",
    "  non_furniture_count (int): Number of non-furniture items in the dataset.\n",
    "  total_count (int): Total number of items in the dataset.\n",
    "  \"\"\"\n",
    "  def __init__(self, tokenized_furniture_names, tokenized_non_furniture_names):\n",
    "    \"\"\"\n",
    "    Initialize the dataset with tokenized furniture and non-furniture names.\n",
    "\n",
    "    Args:\n",
    "    tokenized_furniture_names (dict): A dictionary containing the tokenized names of furniture products.\n",
    "    tokenized_non_furniture_names (dict): A dictionary containing the tokenized names of non-furniture products.\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    \n",
    "    self.tokenized_furniture_names = tokenized_furniture_names\n",
    "    self.tokenized_non_furniture_names = tokenized_non_furniture_names\n",
    "\n",
    "    # Count of furniture and non-furniture items in the dataset\n",
    "    self.furniture_count = len(self.tokenized_furniture_names[\"input_ids\"])\n",
    "    self.non_furniture_count = len(self.tokenized_non_furniture_names[\"input_ids\"])\n",
    "    self.total_count = self.furniture_count + self.non_furniture_count\n",
    "  \n",
    "  def __len__(self):\n",
    "    \"\"\"\n",
    "    Return the total count of items in the dataset.\n",
    "    \"\"\"\n",
    "    return self.total_count\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    \"\"\"\n",
    "    Retrieve an item from the dataset at the specified index.\n",
    "\n",
    "    Args:\n",
    "    index (int): Index of the item to be retrieved.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing input_ids, attention_mask, and label for the item.\n",
    "    \"\"\"\n",
    "    if index < self.furniture_count:\n",
    "        input_ids = self.tokenized_furniture_names[\"input_ids\"][index]\n",
    "        attention_mask = self.tokenized_furniture_names[\"attention_mask\"][index]\n",
    "        label = 1\n",
    "    else:\n",
    "        index -= self.furniture_count\n",
    "        input_ids = self.tokenized_non_furniture_names[\"input_ids\"][index]\n",
    "        attention_mask = self.tokenized_non_furniture_names[\"attention_mask\"][index]\n",
    "        label = 0\n",
    "\n",
    "    return {\n",
    "      \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "      \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
    "      \"label\": torch.tensor(label, dtype=torch.long),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_furniture_names = tokenizer(furniture_names)\n",
    "tokenized_other_product_names = tokenizer(other_product_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = FurnitureDataset(\n",
    "  tokenized_furniture_names,\n",
    "  tokenized_other_product_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_RATIO = 0.8\n",
    "VAL_RATIO = 0.1\n",
    "\n",
    "train_size = int(TRAIN_RATIO * len(dataset))\n",
    "val_size = int(VAL_RATIO * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # Device configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment():\n",
    "    \"\"\"\n",
    "    Conducts an experiment by training, validating, and testing a DistilBERT model for sequence classification.\n",
    "\n",
    "    The function performs the following steps:\n",
    "    1. Initializes data loaders for training, validation, and testing datasets.\n",
    "    2. Sets up the DistilBERT model, optimizer, and loss function.\n",
    "    3. Runs training and validation for a specified number of epochs.\n",
    "    4. Evaluates the model on the test dataset.\n",
    "    5. Saves the model\n",
    "\n",
    "    No parameters are taken; instead, the function uses pre-defined settings.\n",
    "    \"\"\"\n",
    "\n",
    "    # Data loading and model configuration parameters\n",
    "    batch_size = 64\n",
    "    batch_log_count = 25\n",
    "    num_epochs = 5\n",
    "    best_vloss = 1_000_000.  # Initial best validation loss for comparison\n",
    "\n",
    "    # Initializing data loaders for training, validation, and testing\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, collate_fn=data_collator, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, collate_fn=data_collator, shuffle=False)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, collate_fn=data_collator, shuffle=False)\n",
    "\n",
    "    # Initializing model and moving it to the appropriate device\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "    model.to(device)\n",
    "\n",
    "    # Setting up the optimizer and loss function\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=3e-5)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # TensorBoard for logging\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    writer = SummaryWriter('../experiment_logs/furniture_trainer_{}'.format(timestamp))\n",
    "\n",
    "    # Training and validation loop\n",
    "    epoch_number = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"EPOCH {epoch_number}:\")\n",
    "\n",
    "        # Training phase\n",
    "        model.train(True)\n",
    "        # Initialize metrics for training\n",
    "        avg_loss, avg_accuracy, avg_precision, avg_recall, avg_f1 = 0, 0, 0, 0, 0\n",
    "        for i, data in enumerate(train_dataloader):\n",
    "            current_step = epoch * len(train_dataloader) + i + 1\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Prepare data and perform a forward pass\n",
    "            input_ids = data[\"input_ids\"].to(device)\n",
    "            attention_mask = data[\"attention_mask\"].to(device)\n",
    "            labels = data[\"labels\"].to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = loss_fn(outputs.logits, labels)\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Update model parameters\n",
    "\n",
    "            # Update training metrics\n",
    "            avg_loss += loss.item()\n",
    "            avg_accuracy += (outputs.logits.argmax(axis=1) == labels).sum().item() / len(labels)\n",
    "            avg_precision += ((outputs.logits.argmax(axis=1) == labels) & (labels == 1)).sum().item() / (outputs.logits.argmax(axis=1) == 1).sum().item()\n",
    "            avg_recall += ((outputs.logits.argmax(axis=1) == labels) & (labels == 1)).sum().item() / (labels == 1).sum().item()\n",
    "            avg_f1 = 2 * avg_precision * avg_recall / (avg_precision + avg_recall)\n",
    "\n",
    "            # Log metrics every `batch_log_count` batches\n",
    "            if i % batch_log_count == batch_log_count - 1:\n",
    "                avg_loss /= batch_log_count\n",
    "                avg_accuracy /= batch_log_count\n",
    "                avg_precision /= batch_log_count\n",
    "                avg_recall /= batch_log_count\n",
    "                avg_f1 /= batch_log_count\n",
    "\n",
    "                print(f\"  batch {i + 1} loss: {avg_loss}, accuracy: {avg_accuracy}, recall: {avg_recall}, precision: {avg_precision}, f1: {avg_f1}\")\n",
    "\n",
    "                # Writing metrics to TensorBoard\n",
    "                writer.add_scalar('Loss/train', avg_loss, current_step)\n",
    "                writer.add_scalar('Accuracy/train', avg_accuracy, current_step)\n",
    "                writer.add_scalar('Precision/train', avg_precision, current_step)\n",
    "                writer.add_scalar('Recall/train', avg_recall, current_step)\n",
    "                writer.add_scalar('F1/train', avg_f1, current_step)\n",
    "\n",
    "                # Reset metrics after logging\n",
    "                avg_loss, avg_accuracy, avg_precision, avg_recall, avg_f1 = 0, 0, 0, 0, 0\n",
    "\n",
    "        # Validation phase\n",
    "        running_vloss = 0\n",
    "        val_true_positives, val_false_positives, val_true_negatives, val_false_negatives = 0, 0, 0, 0\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        with torch.no_grad():  # Disable gradient calculation\n",
    "            for i, vdata in enumerate(val_dataloader):\n",
    "                # Prepare validation data\n",
    "                input_ids = vdata[\"input_ids\"].to(device)\n",
    "                attention_mask = vdata[\"attention_mask\"].to(device)\n",
    "                labels = vdata[\"labels\"].to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                voutputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                vloss = loss_fn(voutputs.logits, labels)\n",
    "\n",
    "                # Update validation metrics\n",
    "                val_true_positives += ((voutputs.logits.argmax(axis=1) == labels) & (labels == 1)).sum().item()\n",
    "                val_false_positives += ((voutputs.logits.argmax(axis=1) == 1) & (labels == 0)).sum().item()\n",
    "                val_true_negatives += ((voutputs.logits.argmax(axis=1) == labels) & (labels == 0)).sum().item()\n",
    "                val_false_negatives += ((voutputs.logits.argmax(axis=1) == 0) & (labels == 1)).sum().item()\n",
    "\n",
    "                running_vloss += vloss.item()\n",
    "\n",
    "        # Calculate average validation loss and metrics\n",
    "        avg_vloss = running_vloss / (i + 1)\n",
    "        val_accuracy = (val_true_positives + val_true_negatives) / (val_true_positives + val_true_negatives + val_false_positives + val_false_negatives)\n",
    "        val_precision = val_true_positives / (val_true_positives + val_false_positives)\n",
    "        val_recall = val_true_positives / (val_true_positives + val_false_negatives)\n",
    "        val_f1 = 2 * val_precision * val_recall / (val_precision + val_recall)\n",
    "\n",
    "        print(f\"LOSS train {avg_loss} valid {avg_vloss} accuracy {val_accuracy} precision {val_precision} recall {val_recall} f1 {val_f1}\")\n",
    "\n",
    "        # Writing validation metrics to TensorBoard\n",
    "        writer.add_scalar('Loss/valid', avg_vloss, epoch_number)\n",
    "        writer.add_scalar('Accuracy/valid', val_accuracy, epoch_number)\n",
    "        writer.add_scalar('Precision/valid', val_precision, epoch_number)\n",
    "        writer.add_scalar('Recall/valid', val_recall, epoch_number)\n",
    "        writer.add_scalar('F1/valid', val_f1, epoch_number)\n",
    "        writer.flush()\n",
    "\n",
    "        # Model checkpointing based on validation loss\n",
    "        if avg_vloss < best_vloss:\n",
    "            best_vloss = avg_vloss\n",
    "            model_path = f\"../models/model_{timestamp}_{epoch_number}\"\n",
    "            model.save_pretrained(model_path)  # Save the model\n",
    "\n",
    "        epoch_number += 1\n",
    "    \n",
    "    # Test phase\n",
    "    test_true_positives, test_false_positives, test_true_negatives, test_false_negatives = 0, 0, 0, 0\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for i, tdata in enumerate(test_dataloader):\n",
    "            # Prepare test data\n",
    "            input_ids = tdata[\"input_ids\"].to(device)\n",
    "            attention_mask = tdata[\"attention_mask\"].to(device)\n",
    "            labels = tdata[\"labels\"].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            toutputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            # Update test metrics\n",
    "            test_true_positives += ((toutputs.logits.argmax(axis=1) == labels) & (labels == 1)).sum().item()\n",
    "            test_false_positives += ((toutputs.logits.argmax(axis=1) == 1) & (labels == 0)).sum().item()\n",
    "            test_true_negatives += ((toutputs.logits.argmax(axis=1) == labels) & (labels == 0)).sum().item()\n",
    "            test_false_negatives += ((toutputs.logits.argmax(axis=1) == 0) & (labels == 1)).sum().item()\n",
    "\n",
    "    # Calculate and print test metrics\n",
    "    test_accuracy = (test_true_positives + test_true_negatives) / (test_true_positives + test_true_negatives + test_false_positives + test_false_negatives)\n",
    "    test_precision = test_true_positives / (test_true_positives + test_false_positives)\n",
    "    test_recall = test_true_positives / (test_true_positives + test_false_negatives)\n",
    "    test_f1 = 2 * test_precision * test_recall / (test_precision + test_recall)\n",
    "    print(f\"TEST accuracy {test_accuracy} precision {test_precision} recall {test_recall} f1 {test_f1}\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0:\n",
      "  batch 25 loss: 0.35609730005264284, accuracy: 0.886875, recall: 0.9612036121765698, precision: 0.8902863505458195, f1: 0.9243868162891546\n",
      "  batch 50 loss: 0.03394752856343985, accuracy: 0.993125, recall: 0.9941143321762516, precision: 0.9923325785159399, f1: 0.9932226562695916\n",
      "  batch 75 loss: 0.03331039322540164, accuracy: 0.9925, recall: 0.9949285714285715, precision: 0.9910720289667658, f1: 0.9929965557577735\n",
      "  batch 100 loss: 0.014766490627080202, accuracy: 0.9975, recall: 0.9978235294117647, precision: 0.9974901960784313, f1: 0.9976568349020803\n",
      "  batch 125 loss: 0.008649311624467373, accuracy: 0.99875, recall: 0.9987096774193549, precision: 0.9987096774193549, f1: 0.998709677419355\n",
      "LOSS train 0.21713808667846024 valid 0.003950648203802605 accuracy 0.9991079393398751 precision 0.9982332155477032 recall 1.0 f1 0.9991158267020336\n",
      "EPOCH 1:\n",
      "  batch 25 loss: 0.008238663766533136, accuracy: 0.99875, recall: 1.0, precision: 0.9975555555555554, f1: 0.9987762821225944\n",
      "  batch 50 loss: 0.005972009962424636, accuracy: 0.99875, recall: 0.9986206896551724, precision: 0.9988888888888888, f1: 0.9987547712669033\n",
      "  batch 75 loss: 0.0023507888615131377, accuracy: 0.999375, recall: 1.0, precision: 0.9988888888888888, f1: 0.9994441356309061\n",
      "  batch 100 loss: 0.0011792122106999159, accuracy: 1.0, recall: 1.0, precision: 1.0, f1: 1.0\n",
      "  batch 125 loss: 0.0008353155129589141, accuracy: 1.0, recall: 1.0, precision: 1.0, f1: 1.0\n",
      "LOSS train 0.1119370908709243 valid 0.0026482645836141375 accuracy 0.9991079393398751 precision 0.9982332155477032 recall 1.0 f1 0.9991158267020336\n",
      "EPOCH 2:\n",
      "  batch 25 loss: 0.0007054222677834332, accuracy: 1.0, recall: 1.0, precision: 1.0, f1: 1.0\n",
      "  batch 50 loss: 0.0090638526994735, accuracy: 0.99875, recall: 0.9986666666666666, precision: 0.9988888888888888, f1: 0.9987777654169911\n",
      "  batch 75 loss: 0.0007926192763261497, accuracy: 1.0, recall: 1.0, precision: 1.0, f1: 1.0\n",
      "  batch 100 loss: 0.000671084423083812, accuracy: 1.0, recall: 1.0, precision: 1.0, f1: 1.0\n",
      "  batch 125 loss: 0.0005346280278172344, accuracy: 1.0, recall: 1.0, precision: 1.0, f1: 1.0\n",
      "LOSS train 0.007635678804945201 valid 0.0016132985652398525 accuracy 0.9991079393398751 precision 0.9982332155477032 recall 1.0 f1 0.9991158267020336\n",
      "EPOCH 3:\n",
      "  batch 25 loss: 0.0003933248878456652, accuracy: 1.0, recall: 1.0, precision: 1.0, f1: 1.0\n",
      "  batch 50 loss: 0.0003354502539150417, accuracy: 1.0, recall: 1.0, precision: 1.0, f1: 1.0\n",
      "  batch 75 loss: 0.0024947055522352457, accuracy: 0.999375, recall: 0.998918918918919, precision: 1.0, f1: 0.9994591671173607\n",
      "  batch 100 loss: 0.00043582032551057635, accuracy: 1.0, recall: 1.0, precision: 1.0, f1: 1.0\n",
      "  batch 125 loss: 0.012596684540621936, accuracy: 0.9975, recall: 0.9988888888888888, precision: 0.9966432778932779, f1: 0.9977648198763107\n",
      "LOSS train 0.3150159166834783 valid 0.02811707153644723 accuracy 0.9928635147190009 precision 0.9860383944153578 recall 1.0 f1 0.992970123022847\n",
      "EPOCH 4:\n",
      "  batch 25 loss: 0.011920620477758348, accuracy: 0.99625, recall: 0.9954901960784314, precision: 0.9979480519480519, f1: 0.996717608778074\n",
      "  batch 50 loss: 0.0029994527180679143, accuracy: 0.99875, recall: 0.9989743589743589, precision: 0.9987096774193549, f1: 0.9988420006624709\n",
      "  batch 75 loss: 0.00047518597799353304, accuracy: 1.0, recall: 1.0, precision: 1.0, f1: 1.0\n",
      "  batch 100 loss: 0.004523194879293441, accuracy: 0.99875, recall: 0.9986666666666667, precision: 0.9988888888888888, f1: 0.9987777654169911\n",
      "  batch 125 loss: 0.000803785400930792, accuracy: 1.0, recall: 1.0, precision: 1.0, f1: 1.0\n",
      "LOSS train 0.006127384534920566 valid 0.0054049468380981125 accuracy 0.9982158786797503 precision 0.9964726631393298 recall 1.0 f1 0.9982332155477032\n",
      "TEST accuracy 1.0 precision 1.0 recall 1.0 f1 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(string, model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_ids = torch.tensor(tokenizer(string)[\"input_ids\"]).unsqueeze(0).to(device)\n",
    "        attention_mask = torch.tensor(tokenizer(string)[\"attention_mask\"]).unsqueeze(0).to(device)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        is_furniture = outputs.logits.argmax(axis=1).item()\n",
    "        print(f\"Is furniture: {'yes' if is_furniture else 'no'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"../models/model_20231204_195024_4/\").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is furniture: yes\n"
     ]
    }
   ],
   "source": [
    "predict(\"MANHATTAN | BLACK PVC DINING CHAIRS | SET OF 4\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
